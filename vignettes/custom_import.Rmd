---
title: "Custom workflow using low-level parsers"
author: "Hugo Gruson"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Some use cases require more flexibility than the high-level user-friendly
functions provides by `lightr`. For this use case, `lightr` also exports the
low-level individual parsers, which allow the user to code its own custom
workflow.

We don't recommend the use of those functions unless you absolutely have to.
Most users should use `get_spec()` and `get_metadata()` instead.

A common request from spectral data users is too keep their raw data, without
any interpolation. This is not possible in `get_spec()` but it parses spectra
from many different formats and then concatenates them to output a single 
dataframe. For this to be possible, all spectra must be evaluated over the same
wavelengths, which is not usually the case at first. So, we do need to 
interpolate them to make sure that they can safely be concatenated afterwards.

In this vignette, I described how you can import your spectral data without any
interpolation in the case where you are working with only one file format,
created from the same spectrometer and software.

## Step 1: find all files

```{r}
jdx_files <- list.files("data/heliomaster", pattern = "jdx$", full.names = TRUE)
```

## Step 2: import individual spectra

```{r}
first_jdx <- parse_jdx(jdx_files[1])[[1]]
head(first_jdx)
```

As you can see on this first file, `parse_$extension()` functions return a
data frame with many columns. The meaning of each column is explained in full
details in `?parse_jdx`. Here, we are only interested in the first column (the
wavelengths) and the last one (the normalised spectral data).

```{r}
res <- first_jdx[, c("wl", "processed")]
```

## Step 3: create a loop

We captured the wavelengths in the first spectra, we don't need to save them 
each time because in this example, they are the same for all spectra. So we only
record the "processed" column:

```{r}
for (i in 2:length(jdx_files)) {
  next_jdx <- parse_jdx(jdx_files[i])[[1]]
  
  res <- cbind(res, next_jdx[, "processed"])
}
colnames(res) <- c("wl", paste0("spec", seq_along(jdx_files)))
```

And it's done, we can now convert `res` to an `rspec` object and use it in our
analyses. The spectrometer I used for those measurements is not reliable outside
the 300-700 nm wavelength range so we will only keep this range:


```{r eval=F}
library(pavo)
res <- na.omit(res)
res <- res[res$wl > 300 & res$wl < 700, ]
res <- as.rspec(res, interp = FALSE, whichwl = "wl")

plot(res)
```

## Bonus: one-liner with the tidyverse

```{r, eval = FALSE}
library(tidyverse)
map_dfc(jdx_files, function(file) parse_jdx(file)[[1]]) %>%
  select(wl, starts_with("processed"))
```

## Caveat

The example presented in this example might be useful if you want to export your
data in a human readable format, while retaining as much information as 
possible.

**But be careful if you intend to use this interpolated data in your analyses**.
Many statistics and models will produce bogus results when fed interpolated 
data.

One reason if because wavelengths are not always evenly distributed within the 
sampling range (they can result from a [$3^{th}$](https://oceanoptics.com/wp-content/uploads/Spectrometer-Wavelength-Calibration-Instructions.pdf)
to $5^{th}$ order polynome). Because of this, some regions of the wavelength
range will be more heavily sampled than others, so statistics such a $S1$ 
(`summary.rspec()` in `pavo`) may not make any sense.

